{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9597391",
   "metadata": {},
   "source": [
    "#  Twiiter Sentiment analysis on a presidential aspirant(Peter Obi) using NLP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f35823cc",
   "metadata": {},
   "source": [
    "Notebook by Olutoba Joel Olufisayo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faf695",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae3f1b2d",
   "metadata": {},
   "source": [
    "1.    Introduction\n",
    "2.    Data mining and concatenation\n",
    "3.    Import libraries\n",
    "4.    Data cleaning\n",
    "   ** Data validation\n",
    "   ** Missing Value\n",
    "   ** Handling data types\n",
    "5. Handling of text column\n",
    "   ** Get unique #tags\n",
    "   ** New column for other aspirants mention\n",
    "   ** Remove stopwords, puntuations and apply lematizer\n",
    "   ** Text blob, word correction\n",
    "   ** Remove rare words\n",
    "   ** Generate column for cleaned tweets\n",
    "   ** Generate ratioed column\n",
    "6. Data Exploratory Analysis\n",
    "   **  Chart of top users\n",
    "   ** Chart of users with most likes\n",
    "   ** Charts of users with most retweet and chart of users with most engagement\n",
    "   ** Top ratio tweets\n",
    "   ** Top ratio tweets\n",
    "   ** Chart of other aspirants alonside Peter Obi\n",
    "   ** Charts of top locations\n",
    "   ** Word cloud\n",
    "7. Sentiment analysis\n",
    "   ** Subjectivity\n",
    "   ** Polarity\n",
    "   ** Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba80fa1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a Data Analytics Project focused on Peter Obi tweets (presidental aspirant in Nigeria) using Natural Language Processing (NLP) Techniques. \n",
    "Every 4 year in Nigeria there is a change of govenmnet or extension of existing one, where every polictical party present their candidates, Peter Obi as one of the candidtes who seem to be widely talked about on twitter after he declared his ambition on one of the political parties. \n",
    "\n",
    "Sncrape was used to mine tweets about Peter Obi from March 2022 shortly before he declared his ambition till August 2022, the total tweets mined was 387,109k. This Notebook makes use of several Python libraries like Pandas (for Data Manipulation/Cleaning), Sncrape (for Tweets Mining), NLTK (Natural Language Toolkit libray to handle text data), TextBlob (for Sentiment Analysis), MatPlotlib, Plotly & WordCloud (for Data Visualization), Emot & emoticin (for Emojis identification/for expressed emotion emoji).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb60572",
   "metadata": {},
   "source": [
    "# Data mining and concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8163e",
   "metadata": {},
   "source": [
    "Sncrape as a minning tool is as easy as bitting a pie, after trying a couple of tools including tweepy which requires regorous process for getting a substantial amount of data from twitter.\n",
    "Snscrape is a scraper for social networking services (SNS), including facebook, Instagram etc. With it we were able to mine tweets containing 'peter obi' as our key word, data for six months was scrapped, i.e from March 4th 2022 till August 4th 2022, the data contains datatime, username, follower counts, like count, and more other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e051a54",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "Import all neccesary libraries to be used on this project. It is a convention way of importing the dependencies needed to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafdade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweepy # for tweet mining\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv # to read and write csv files\n",
    "import re # In-built regular expressions library\n",
    "import string # Inbuilt sting library\n",
    "import glob # to retrieve files/pathnames matching a specified pattern. \n",
    "import random # generating random numbers\n",
    "import requests # to send HTTP requests\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "from PIL import Image # for opening, manipulating, and saving many different image file f\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "from nltk.corpus import stopwords # get stopwords from NLTK library\n",
    "from nltk.tokenize import word_tokenize # to create word tokens\n",
    "from nltk.stem.porter import * # (I played around with Stemmer and decided to use Lemmatizer instead)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer # to reduce words to orginal form\n",
    "from nltk import pos_tag # For Parts of Speech tagging\n",
    "from nltk.corpus import words # Get all words in english language\n",
    "\n",
    "#Text Sentiments\n",
    "#TextBlob - Python library for processing textual data\n",
    "import textblob\n",
    "from textblob import TextBlob \n",
    "\n",
    "#Visualization tools\n",
    "import plotly.express as px # To make express plots in Plotly\n",
    "import chart_studio.tools as cst # For exporting to Chart studio\n",
    "import chart_studio.plotly as py # for exporting Plotly visualizations to Chart Studio\n",
    "import plotly.offline as pyo # Set notebook mode to work in offline\n",
    "pyo.init_notebook_mode()\n",
    "import plotly.io as pio # Plotly renderer\n",
    "import plotly.graph_objects as go # For plotting plotly graph objects\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# WordCloud - Python linrary for creating image wordclouds\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
    "\n",
    "import chart_studio\n",
    "username='olujoe'\n",
    "api_key='your_api_key'\n",
    "chart_studio.tools.set_credentials_file(username=username,\n",
    "                                        api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7634bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "tweets_df = pd.read_csv('peter_obi_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the data shape and view the dataframe\n",
    "print('shape:', tweets_df.shape) \n",
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ec443",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99270df2",
   "metadata": {},
   "source": [
    "through data cleaning proccess we are able to check for nan and none values in the dataframe, we wrw able to modify the data to ensure it is free of irrelevances and incorrect information. Date time was created to for an insight into the daily data trend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Remove the unnamed column\n",
    "tweets_df.drop(['Unnamed: 0'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values, no missing value, as our data is free of nan values\n",
    "tweets_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7591ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statitiscal insight into the data\n",
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ba4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info about our data\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp column to datetime type\n",
    "tweets_df['Datetime'] = pd.to_datetime(tweets_df['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date and time to create new columns \n",
    "tweets_df['Date'] = pd.to_datetime(tweets_df['Datetime']).dt.date\n",
    "tweets_df['Time'] = pd.to_datetime(tweets_df['Datetime']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08233fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Date'] = pd.to_datetime(tweets_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f544620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for the newly added columns\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb1e5c",
   "metadata": {},
   "source": [
    "# Handling text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41d34a",
   "metadata": {},
   "source": [
    "Text data i.e individual tweet contains charaters(emojis, numbers, #tags, unfamiliar words and stopwords) that can serve as noise in our data, which make it important for our data to be cleaned and properly treated. Some of the above libraries like NLTK helped us to manipulate our string data by changing the texts data into lower case, remove stopwaords, create additional columns for insight, remove emojis and lemmetize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for getting unique hashtags in the dataframe\n",
    "def getHashtags(tweet):\n",
    "    tweet = tweet.lower()  #has to be in place\n",
    "    tweet = re.findall(r'\\#\\w+',tweet) # Remove hastags with REGEX\n",
    "    return \" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create #tag relative to each tweet.\n",
    "tweets_df['Hashtags'] = tweets_df['Tweets'].apply(getHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e908cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list for other aspirants and aliases\n",
    "\n",
    "other_aspirants = ['tinubu','asiwaju','bat','agbado','emi lokan', 'atiku', 'bobo chicago', 'jagaban', 'city boy',\n",
    "                      'wike', 'kwankwaso', 'osinbajo', 'wike','saraki',\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d45814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for generating other aspirants mentioned in peter_obi tweets\n",
    "\n",
    "def get_other_aspirants(tweet):\n",
    "    tweet = tweet.lower() # Reduce tweet to lower case\n",
    "    tweet_tokens = word_tokenize(tweet) # split each word in the tweet for parsing\n",
    "    presidential_aspirants = [char for char in tweet_tokens if char in other_aspirants]\n",
    "    return \" \".join(presidential_aspirants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Aspirants_mentioned'] = tweets_df['Tweets'].apply(get_other_aspirants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate ratio column\n",
    "\n",
    "tweets_df['Ratioed'] = np.where(tweets_df['Replies_count'] > tweets_df['Likes_count'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44075a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  remove stopwords or some very common word manually and punctuation/ apply lemmatizer\n",
    "\n",
    "# Defining my NLTK stop words and my user-defined stop words\n",
    "stop_words = list(stopwords.words('english'))\n",
    "user_stop_words = ['2023', '2019', 'much','pvc','president', 'abeg','presidential', 'election', 'amp', 'next', 'cant', 'wont', 'hadnt','havent', 'hasnt', \n",
    "                   'isnt', 'shouldnt', 'couldnt', 'wasnt', 'werent','mustnt', '’', '...', '..', '.', '.....', '....', \n",
    "                   'been…','aht', 've', 'next',\"i'm\",\"i'll\",\"we'll\",\"they'll\",\"you'll\",\"she'll\",\"he'll\",\"'ll\",\"n't\",\n",
    "                   \"'s\",'anyone','today', 'lmao', 'lwkm','lol','yesterday','day', 'already']\n",
    "\n",
    "alphabets = list(string.ascii_lowercase)\n",
    "stop_words = stop_words + user_stop_words + alphabets\n",
    "#word_list = words.words()  # all words in English language\n",
    "emojis = list(UNICODE_EMOJI.keys())  # full list of emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words, punctuations, emojis and return words to their base form using Lemmatizer\n",
    "\n",
    "def preprocessTweets(tweet):\n",
    "    tweet = tweet.lower()  # changes all words to lower case\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags = re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#\\w+|\\d+', '', tweet)\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)  # convert string to tokens\n",
    "    filtered_words = [w for w in tweet_tokens if w not in stop_words]\n",
    "    filtered_words = [w for w in filtered_words if w not in emojis]\n",
    "    \n",
    "    # Remove punctuations\n",
    "    unpunctuated_words = [w for w in filtered_words if w not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer() # instatiate an object WordNetLemmatizer Class\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in unpunctuated_words]\n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preProcessTweets function to the 'Tweet' column to generate a new column called 'Processed Tweets'.\n",
    "# This returns all words used to describe the TV series except for words in the lists above\n",
    "tweets_df['Cleaned_Tweets'] = tweets_df['Tweets'].apply(preprocessTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07563bef",
   "metadata": {},
   "source": [
    "# Data exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cb6e",
   "metadata": {},
   "source": [
    "### Charting and Visualization\n",
    "\n",
    "In this section plotly was chosen as the vissualisation tool becuase of it flexibily and reactive screen, indights were gotten from the variables and a wordcloud was generated with the twitter bird ping to display the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart for counts of users with most tweets\n",
    "top_user = tweets_df.Username.value_counts().nlargest(10).to_frame()\n",
    "top_user['name'] = top_user.index\n",
    "top_user['Count'] = top_user['Username']\n",
    "px.bar(top_user, x = 'name', y = 'Count', title = 'COUNT OF USERS WITH MOST TWEETS', color_discrete_sequence = ['darkred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f370d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart for counts of users with most likes\n",
    "max_likes =tweets_df.loc[tweets_df['Likes_count'] >= 30000]\n",
    "px.bar(max_likes, x = 'Username', y = 'Likes_count', title = 'USERS WITH THE MOST LIKES', color_discrete_sequence = ['green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart for counts of users with most retweets\n",
    "max_retweets =tweets_df.loc[tweets_df['Retweet_count'] >= 7000]\n",
    "px.bar(max_retweets, x = 'Username', y = 'Retweet_count', title = 'USERS WITH THE MOST RETWEET',color_discrete_sequence = ['mediumvioletred'], color = 'Ratioed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart for counts of users with most engagement\n",
    "tweets_df['Engagement'] = tweets_df['Likes_count'] + tweets_df['Retweet_count'] + tweets_df['Replies_count'] + tweets_df['Quotes_count']\n",
    "#tweets_df.head(5)\n",
    "max_engagement =tweets_df.loc[tweets_df['Engagement'] >= 30000]\n",
    "px.bar(max_engagement, x = 'Username', y = 'Engagement', title = 'USERS WITH THE MOST ENGAGEMENT', color = 'Ratioed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd44cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart for counts of users with most engagement and ratio\n",
    "ratio_df =tweets_df.loc[tweets_df['Ratioed'] == 'Yes']\n",
    "#ratio_df.head(5)\n",
    "max_ratio = ratio_df.loc[ratio_df['Engagement'] >= 2000]\n",
    "\n",
    "px.bar(max_ratio, x = 'Username', y = 'Engagement', title = 'USERS WITH THE MOST RATIO', color_discrete_sequence = ['darkred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57443b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a daytime column for day\n",
    "tweets_df['Day'] = tweets_df['Date'].dt.day_name()\n",
    "# most active days with engagement \n",
    "colors = ['blue','green','black','purple','red', 'brown','cyan']\n",
    "fig = px.pie(tweets_df, names = 'Day', title = 'MOST ACTIVE DAYS', color_discrete_sequence = ['green'])\n",
    "\n",
    "fig.update_traces(hoverinfo = 'label + percent',textfont_size = 20, \n",
    "                 textinfo = 'label + percent', pull = [0.1,0,0,0,0,0,0],\n",
    "                 marker = dict(colors = colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b52bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tweets into one long string with each word separate with a \"space\"\n",
    "tweets_long_string = tweets_df['Cleaned_Tweets'].tolist()\n",
    "tweets_long_string = \" \".join(tweets_long_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# join tweets to a single string\n",
    "words = ' '.join(tweets_df['Cleaned_Tweets'])\n",
    "\n",
    "# remove URLs, RTs, and twitter handles\n",
    "neww = \" \".join([word for word in words.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "# create a twitter-style mask for the wordcloud\n",
    "twitter_mask = imageio.imread('kfc.png')\n",
    "\n",
    "\n",
    "STOPWORDS = stop_words\n",
    "\n",
    "# generate the wordcloud\n",
    "wordcloud = WordCloud(\n",
    "                    stopwords = STOPWORDS,\n",
    "                    background_color='white',\n",
    "                    width=1800, \n",
    "                    height=1400, \n",
    "                    mask=twitter_mask\n",
    "                    ).generate(neww)\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "# show the wordcloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a6c165",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613db439",
   "metadata": {},
   "source": [
    "Finally we built a rule based sentiment analysis function to classify the sentiment behind the tweets; \n",
    "\n",
    "Polarity – talks about how positive or negative the opinion is\n",
    "\n",
    "Subjectivity – talks about how subjective the opinion is\n",
    "\n",
    "TextBlob(cleaned_tweets).sentiment gives us the Polarity, Subjectivity values.\n",
    "\n",
    "Polarity ranges from -1 to 1 (1 is more positive, 0 is neutral, -1 is more negative)\n",
    "\n",
    "Subjectivity ranges from 0 to 1(0 being very objective and 1 being very subjective)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function to calculate subjrctivity\n",
    "def getSubjectivity(cleaned_tweets):\n",
    "    return TextBlob(cleaned_tweets).sentiment.subjectivity\n",
    "#function to calculate sentiment\n",
    "def getPolarity(cleaned_tweets):\n",
    "    return TextBlob(cleaned_tweets).sentiment.polarity\n",
    "\n",
    "#function to analyse the cleaned tweets\n",
    "def analysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutra'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for our cleaned_text data\n",
    "sentiment_analysis = pd.DataFrame(tweets_df[['Cleaned_Tweets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ed296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_analysis['Subjectivity'] = sentiment_analysis['Cleaned_Tweets'].apply(getSubjectivity)\n",
    "sentiment_analysis['Polarity'] = sentiment_analysis['Cleaned_Tweets'].apply(getPolarity)\n",
    "sentiment_analysis['Analysis'] = sentiment_analysis['Polarity'].apply(analysis)\n",
    "sentiment_analysis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "colors = ['blue','green','black','purple','red', 'brown','cyan']\n",
    "fig = px.pie(sentiment_analysis, names = 'Analysis', title = 'PIE CHART OF SENTIMENT', color_discrete_sequence = ['green'])\n",
    "\n",
    "fig.update_traces(hoverinfo = 'label + percent',textfont_size = 20,\n",
    "                 textinfo = 'label + percent', pull = [0.1,0,0,0,0,0,0],\n",
    "                 marker = dict(colors = colors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
